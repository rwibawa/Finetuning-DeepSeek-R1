{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tuning DeepSeek R1 (Reasoning Model)\nFine-tuning the world's first open-source reasoning model on the medical chain of thought dataset to build better AI doctors for the future.\n\nReference:\n* [Tutorial](https://www.datacamp.com/tutorial/fine-tuning-deepseek-r1-reasoning-model)\n\n## 1. Setting Up\nFor this project, we are using Kaggle as our Cloud IDE because it provides free access to GPUs, which are often more powerful than those available in Google Colab. To get started, launch a new Kaggle notebook and add your Hugging Face token and Weights & Biases token as secrets.\n\nYou can add secrets by navigating to the `Add-ons` tab in the Kaggle notebook interface and selecting the `Secrets` option.\n* **HF_TOKEN**: the Hugging Face token\n* **wnb**: the Weight & Bias token\n\nAfter setting up the secrets, install the unsloth Python package. Unsloth is an open-source framework designed to make fine-tuning large language models (LLMs) 2X faster and more memory-efficient.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n\n!pip install unsloth # install unsloth\n# Also get the latest nightly Unsloth!\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T21:34:16.590932Z","iopub.execute_input":"2025-03-10T21:34:16.591276Z","iopub.status.idle":"2025-03-10T21:34:31.715425Z","shell.execute_reply.started":"2025-03-10T21:34:16.591251Z","shell.execute_reply":"2025-03-10T21:34:31.714307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Moduls for fine-tuning\nfrom unsloth import FastLanguageModel\nimport torch # Import PyTorch\nfrom trl import SFTTrainer # Trainer for supervised fine-tuning (SFT)\nfrom unsloth import is_bfloat16_supported # Checks if the hardware supports bfloat16 precision\n\nmax_seq_length = 2048 \ndtype = None \nload_in_4bit = True ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T21:37:29.481694Z","iopub.execute_input":"2025-03-10T21:37:29.482139Z","iopub.status.idle":"2025-03-10T21:37:29.487486Z","shell.execute_reply.started":"2025-03-10T21:37:29.482094Z","shell.execute_reply":"2025-03-10T21:37:29.486630Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hugging Face modules\nfrom huggingface_hub import login # Let's you login to API\nfrom transformers import TrainingArguments # Defines training hyperparameters\nfrom datasets import load_dataset # Let's you load fine-tuning datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:00:20.808628Z","iopub.execute_input":"2025-03-10T22:00:20.808907Z","iopub.status.idle":"2025-03-10T22:00:20.812540Z","shell.execute_reply.started":"2025-03-10T22:00:20.808888Z","shell.execute_reply":"2025-03-10T22:00:20.811775Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Log in to the Hugging Face CLI using the Hugging Face API that we securely extracted from Kaggle Secrets. ","metadata":{}},{"cell_type":"code","source":"# Import weights and biases\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:02:21.753108Z","iopub.execute_input":"2025-03-10T22:02:21.753600Z","iopub.status.idle":"2025-03-10T22:02:22.410200Z","shell.execute_reply.started":"2025-03-10T22:02:21.753568Z","shell.execute_reply":"2025-03-10T22:02:22.409540Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Log in to Weights & Biases (`wanb`) using your API key and create a new project to track the experiments and fine-tuning progress.","metadata":{}},{"cell_type":"code","source":"import wandb\n\nwb_token = user_secrets.get_secret(\"wnb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune-DeepSeek-R1-Distill-Llama-8B on Medical COT Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:05:41.883423Z","iopub.execute_input":"2025-03-10T22:05:41.883716Z","iopub.status.idle":"2025-03-10T22:05:55.515199Z","shell.execute_reply.started":"2025-03-10T22:05:41.883694Z","shell.execute_reply":"2025-03-10T22:05:55.514339Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Loading the model and tokenizer\nFor this project, we are loading the Unsloth version of [DeepSeek-R1-Distill-Llama-8B](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B).  Additionally, we will load the model in 4-bit quantization to optimize memory usage and performance.","metadata":{}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\n\nmax_seq_length = 2048 \ndtype = None \nload_in_4bit = True\n\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    token = hf_token, \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:06:29.264665Z","iopub.execute_input":"2025-03-10T22:06:29.264967Z","iopub.status.idle":"2025-03-10T22:07:07.033675Z","shell.execute_reply.started":"2025-03-10T22:06:29.264948Z","shell.execute_reply":"2025-03-10T22:07:07.032772Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Model inference before fine-tuning\nTo create a prompt style for the model, we will define a system prompt and include placeholders for the question and response generation. The prompt will guide the model to think step-by-step and provide a logical, accurate response.","metadata":{}},{"cell_type":"code","source":"prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:07:32.099522Z","iopub.execute_input":"2025-03-10T22:07:32.099955Z","iopub.status.idle":"2025-03-10T22:07:32.105471Z","shell.execute_reply.started":"2025-03-10T22:07:32.099923Z","shell.execute_reply":"2025-03-10T22:07:32.104169Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In this example, we will provide a medical question to the prompt_style, convert it into tokens, and then pass the tokens to the model for response generation. ","metadata":{}},{"cell_type":"code","source":"question = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\n\n\nFastLanguageModel.for_inference(model) \ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:07:41.202455Z","iopub.execute_input":"2025-03-10T22:07:41.202750Z","iopub.status.idle":"2025-03-10T22:08:39.135642Z","shell.execute_reply.started":"2025-03-10T22:07:41.202730Z","shell.execute_reply":"2025-03-10T22:08:39.134747Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Even without fine-tuning, our model successfully generated a chain of thought and provided reasoning before delivering the final answer. The reasoning process is encapsulated within the <think></think> tags.\n\nSo, why do we still need fine-tuning? The reasoning process, while detailed, was long-winded and not concise. Additionally, the final answer was presented in a bullet-point format, which deviates from the structure and style of the dataset that we want to fine-tune on. \n\n## 4. Loading and processing the dataset\nWe will slightly change the prompt style for processing the dataset by adding the third placeholder for the complex chain of thought column. ","metadata":{}},{"cell_type":"code","source":"train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n</think>\n{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:08:53.636644Z","iopub.execute_input":"2025-03-10T22:08:53.636934Z","iopub.status.idle":"2025-03-10T22:08:53.641604Z","shell.execute_reply.started":"2025-03-10T22:08:53.636913Z","shell.execute_reply":"2025-03-10T22:08:53.640795Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Write the Python function that will create a \"text\" column in the dataset, which consists of the train prompt style. Fill the placeholders with questions, chains of text, and answers. ","metadata":{}},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n\n\ndef formatting_prompts_func(examples):\n    inputs = examples[\"Question\"]\n    cots = examples[\"Complex_CoT\"]\n    outputs = examples[\"Response\"]\n    texts = []\n    for input, cot, output in zip(inputs, cots, outputs):\n        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n        texts.append(text)\n    return {\n        \"text\": texts,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:09:09.332754Z","iopub.execute_input":"2025-03-10T22:09:09.333067Z","iopub.status.idle":"2025-03-10T22:09:09.339391Z","shell.execute_reply.started":"2025-03-10T22:09:09.333045Z","shell.execute_reply":"2025-03-10T22:09:09.338296Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will load the first 500 samples from the [FreedomIntelligence/medical-o1-reasoning-SFT](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT?row=46) dataset, which is available on the Hugging Face hub. After that, we will map the text column using the formatting_prompts_func function.\n\nAs we can see, the text column has a system prompt, instructions, chain of thought, and the answer. ","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split = \"train[0:500]\",trust_remote_code=True)\ndataset = dataset.map(formatting_prompts_func, batched = True,)\ndataset[\"text\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:09:23.173533Z","iopub.execute_input":"2025-03-10T22:09:23.173845Z","iopub.status.idle":"2025-03-10T22:09:28.403435Z","shell.execute_reply.started":"2025-03-10T22:09:23.173823Z","shell.execute_reply":"2025-03-10T22:09:28.402731Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Setting up the model\nUsing the target modules, we will set up the model by adding the low-rank adopter to the model. ","metadata":{}},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r=16,  \n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n    ],\n    lora_alpha=16,\n    lora_dropout=0,  \n    bias=\"none\",  \n    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n    random_state=3407,\n    use_rslora=False,  \n    loftq_config=None,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:09:46.669888Z","iopub.execute_input":"2025-03-10T22:09:46.670352Z","iopub.status.idle":"2025-03-10T22:09:53.082217Z","shell.execute_reply.started":"2025-03-10T22:09:46.670315Z","shell.execute_reply":"2025-03-10T22:09:53.081533Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we will set up the training arguments and the trainer by providing the model, tokenizers, dataset, and other important training parameters that will optimize our fine-tuning process.","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n        warmup_steps=5,\n        max_steps=60,\n        learning_rate=2e-4,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=10,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\",\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:10:15.467208Z","iopub.execute_input":"2025-03-10T22:10:15.467536Z","iopub.status.idle":"2025-03-10T22:10:18.214936Z","shell.execute_reply.started":"2025-03-10T22:10:15.467512Z","shell.execute_reply":"2025-03-10T22:10:18.214221Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Model training\nRun the following command to start training.  ","metadata":{}},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:10:27.316996Z","iopub.execute_input":"2025-03-10T22:10:27.317335Z","iopub.status.idle":"2025-03-10T22:49:32.073980Z","shell.execute_reply.started":"2025-03-10T22:10:27.317306Z","shell.execute_reply":"2025-03-10T22:49:32.073375Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You can view the fill model evaluation report on the Weights and biases dash board by logging into the website and viewing the [project](https://wandb.ai/ryan-wibawa-california-state-university/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset?nw=nwuserryanwibawa).\n\n## 7. Model inference after fine-tuning\nTo compare the results, we will ask the fine-tuned model the same question as before to see what has changed.","metadata":{}},{"cell_type":"code","source":"question = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\n\n\nFastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)\nprint(response[0].split(\"### Response:\")[1])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T23:08:32.732699Z","iopub.execute_input":"2025-03-10T23:08:32.733029Z","iopub.status.idle":"2025-03-10T23:09:08.846702Z","shell.execute_reply.started":"2025-03-10T23:08:32.733006Z","shell.execute_reply":"2025-03-10T23:09:08.845694Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This is much better and more accurate. The chain of thought was direct, and the answer was straightforward and in one paragraph. The fine-tuning was successful.\n\n## 8. Saving the model locally\nNow, let's save the adopter, full model, and tokenizer locally so that we can use them in other projects.","metadata":{}},{"cell_type":"code","source":"new_model_local = \"DeepSeek-R1-Medical-COT\"\nmodel.save_pretrained(new_model_local) \ntokenizer.save_pretrained(new_model_local)\n\nmodel.save_pretrained_merged(new_model_local, tokenizer, save_method = \"merged_16bit\",)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T23:11:04.525588Z","iopub.execute_input":"2025-03-10T23:11:04.525913Z","iopub.status.idle":"2025-03-10T23:13:07.275270Z","shell.execute_reply.started":"2025-03-10T23:11:04.525890Z","shell.execute_reply":"2025-03-10T23:13:07.274290Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Pushing the model to Hugging Face Hub\nWe will also push the adopter, tokenizer, and model to Hugging Face Hub so that the AI community can take advantage of this model by integrating it into their systems.","metadata":{}},{"cell_type":"code","source":"new_model_online = \"rwibawa/DeepSeek-R1-Medical-COT\"\nmodel.push_to_hub(new_model_online)\ntokenizer.push_to_hub(new_model_online)\n\nmodel.push_to_hub_merged(new_model_online, tokenizer, save_method = \"merged_16bit\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T23:38:20.681553Z","iopub.execute_input":"2025-03-10T23:38:20.681900Z","iopub.status.idle":"2025-03-10T23:43:33.256396Z","shell.execute_reply.started":"2025-03-10T23:38:20.681871Z","shell.execute_reply":"2025-03-10T23:43:33.255449Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The next step in your learning journey is to serve and deploy your model to the cloud. You can follow the [How to Deploy LLMs with BentoML](https://www.datacamp.com/tutorial/deploy-llms-with-bentoml) guide, which provides a step-by-step process for deploying large language models efficiently and cost-effectively using BentoML and tools like vLLM.\n\nAlternatively, if you prefer to use the model locally, you can convert it into GGUF format and run it on your machine. For this, check out the [Fine-tuning Llama 3.2 and Using It Locally](https://www.datacamp.com/tutorial/fine-tuning-llama-3-2) guide, which provides detailed instructions for local usage.\n\n## 10. Converting the Model to Llama.cpp GGUF\nWe can’t use the safetensors files locally as most local AI chatbots don’t support them. Instead, we'll convert it into the *llama.cpp* GGUF file format.\n\n### Setting up\nInstall the `llama.cpp` by running the following command in the Kaggle Notebook cell.","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone --depth=1 https://github.com/ggerganov/llama.cpp.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T02:01:49.151789Z","iopub.execute_input":"2025-03-12T02:01:49.152142Z","iopub.status.idle":"2025-03-12T02:01:52.018563Z","shell.execute_reply.started":"2025-03-12T02:01:49.152109Z","shell.execute_reply":"2025-03-12T02:01:52.017624Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\nCloning into 'llama.cpp'...\nremote: Enumerating objects: 1369, done.\u001b[K\nremote: Counting objects: 100% (1369/1369), done.\u001b[K\nremote: Compressing objects: 100% (1042/1042), done.\u001b[K\nremote: Total 1369 (delta 297), reused 1025 (delta 280), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (1369/1369), 19.43 MiB | 25.25 MiB/s, done.\nResolving deltas: 100% (297/297), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Build `llama.cpp`","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/llama.cpp\n!sed -i 's|MK_LDFLAGS   += -lcuda|MK_LDFLAGS   += -L/usr/local/nvidia/lib64 -lcuda|' Makefile\n!LLAMA_CUDA=1 make all -j","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T02:10:15.642395Z","iopub.execute_input":"2025-03-12T02:10:15.642808Z","iopub.status.idle":"2025-03-12T02:10:15.914155Z","shell.execute_reply.started":"2025-03-12T02:10:15.642778Z","shell.execute_reply":"2025-03-12T02:10:15.913005Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/llama.cpp\nMakefile:2: *** The Makefile build is deprecated. Use the CMake build instead. For more details, see https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md.  Stop.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### Converting Safetensors to GGUF model format\nRun the following command in the Kaggle Notebook cell to convert the model into the GGUF format.\n\nThe `convert-hf-to-gguf.py` requires an input model directory, output file directory, and out type.","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!python llama.cpp/convert_hf_to_gguf.py DeepSeek-R1-Medical-COT/ \\\n    --outfile /kaggle/working/llama-3-8b-chat-doctor.gguf \\\n    --outtype f16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T02:27:28.600605Z","iopub.execute_input":"2025-03-12T02:27:28.600982Z","iopub.status.idle":"2025-03-12T02:28:37.603565Z","shell.execute_reply.started":"2025-03-12T02:27:28.600957Z","shell.execute_reply":"2025-03-12T02:28:37.602524Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n2025-03-12 02:27:45.970174: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-12 02:27:46.297422: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-12 02:27:46.386570: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nWriting:  26%|██████▉                    | 4.15G/16.1G [00:31<01:34, 126Mbyte/s]Traceback (most recent call last):\n  File \"/kaggle/working/llama.cpp/convert_hf_to_gguf.py\", line 5117, in <module>\n    main()\n  File \"/kaggle/working/llama.cpp/convert_hf_to_gguf.py\", line 5111, in main\n    model_instance.write()\n  File \"/kaggle/working/llama.cpp/convert_hf_to_gguf.py\", line 443, in write\n    self.gguf_writer.write_tensors_to_file(progress=True)\n  File \"/kaggle/working/llama.cpp/gguf-py/gguf/gguf_writer.py\", line 454, in write_tensors_to_file\n    ti.tensor.tofile(fout)\n  File \"/kaggle/working/llama.cpp/gguf-py/gguf/lazy.py\", line 211, in tofile\n    return eager.tofile(*args, **kwargs)\nOSError: Not enough free space to write 117440512 bytes\nWriting:  26%|███████                    | 4.19G/16.1G [00:32<01:31, 130Mbyte/s]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Quantizing the GGUF model\nRegular laptops don’t have enough RAM and GPU memory to load the entire model, so we have to quantify the GGUF model, reducing the 16 GB model to around 4-5 GB.\n\nThe quantize script requires a GGUF model directory, output file directory, and quantization method. We are converting the model using the `Q4_K_M` method.","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/\n\n!./llama.cpp/llama-quantize \\\nllama-3-8b-chat-doctor.gguf \\\nllama-3-8b-chat-doctor-Q4_K_M.gguf \\\nQ4_K_M","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 11.Pushing the model file to Hugging Face\nTo push the single file to the Hugging Face Hub, we'll:\n\n### 1. Login to the Hugging Face Hub using the API key.\n### 2. Create the API object.\n### 3. Upload the file by providing the local path, repo path, repo id, and repo type.","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import HfApi\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(token = hf_token)\n\napi = HfApi()\napi.upload_file(\n    path_or_fileobj=\"/kaggle/working/llama-3-8b-chat-doctor-Q4_K_M.gguf\",\n    path_in_repo=\"llama-3-8b-chat-doctor-Q4_K_M.gguf\",\n    repo_id=\"rwibawa/llama-3-8b-chat-doctor\",\n    repo_type=\"model\",\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\nOpen-source large language models (LLMs) are becoming better, faster, and more efficient, making it easier than ever to fine-tune them on lower compute and memory resources.\n\nIn this tutorial, we explored the DeepSeek R1 reasoning model and learned how to fine-tune its distilled version for medical Q&A tasks. A fine-tuned reasoning model not only enhances performance but also enables its application in critical fields such as medicine, emergency services, and healthcare.","metadata":{}}]}