{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tuning DeepSeek R1 (Reasoning Model)\nFine-tuning the world's first open-source reasoning model on the medical chain of thought dataset to build better AI doctors for the future.\n\nReference:\n* [Tutorial](https://www.datacamp.com/tutorial/fine-tuning-deepseek-r1-reasoning-model)\n\n## 1. Setting Up\nFor this project, we are using Kaggle as our Cloud IDE because it provides free access to GPUs, which are often more powerful than those available in Google Colab. To get started, launch a new Kaggle notebook and add your Hugging Face token and Weights & Biases token as secrets.\n\nYou can add secrets by navigating to the `Add-ons` tab in the Kaggle notebook interface and selecting the `Secrets` option.\n* **HF_TOKEN**: the Hugging Face token\n* **wnb**: the Weight & Bias token\n\nAfter setting up the secrets, install the unsloth Python package. Unsloth is an open-source framework designed to make fine-tuning large language models (LLMs) 2X faster and more memory-efficient.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n\n!pip install unsloth # install unsloth\n# Also get the latest nightly Unsloth!\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T21:34:16.590932Z","iopub.execute_input":"2025-03-10T21:34:16.591276Z","iopub.status.idle":"2025-03-10T21:34:31.715425Z","shell.execute_reply.started":"2025-03-10T21:34:16.591251Z","shell.execute_reply":"2025-03-10T21:34:31.714307Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Moduls for fine-tuning\nfrom unsloth import FastLanguageModel\nimport torch # Import PyTorch\nfrom trl import SFTTrainer # Trainer for supervised fine-tuning (SFT)\nfrom unsloth import is_bfloat16_supported # Checks if the hardware supports bfloat16 precision\n\nmax_seq_length = 2048 \ndtype = None \nload_in_4bit = True ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T21:37:29.481694Z","iopub.execute_input":"2025-03-10T21:37:29.482139Z","iopub.status.idle":"2025-03-10T21:37:29.487486Z","shell.execute_reply.started":"2025-03-10T21:37:29.482094Z","shell.execute_reply":"2025-03-10T21:37:29.486630Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Hugging Face modules\nfrom huggingface_hub import login # Let's you login to API\nfrom transformers import TrainingArguments # Defines training hyperparameters\nfrom datasets import load_dataset # Let's you load fine-tuning datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:00:20.808628Z","iopub.execute_input":"2025-03-10T22:00:20.808907Z","iopub.status.idle":"2025-03-10T22:00:20.812540Z","shell.execute_reply.started":"2025-03-10T22:00:20.808888Z","shell.execute_reply":"2025-03-10T22:00:20.811775Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"Log in to the Hugging Face CLI using the Hugging Face API that we securely extracted from Kaggle Secrets. ","metadata":{}},{"cell_type":"code","source":"# Import weights and biases\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:02:21.753108Z","iopub.execute_input":"2025-03-10T22:02:21.753600Z","iopub.status.idle":"2025-03-10T22:02:22.410200Z","shell.execute_reply.started":"2025-03-10T22:02:21.753568Z","shell.execute_reply":"2025-03-10T22:02:22.409540Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"Log in to Weights & Biases (`wanb`) using your API key and create a new project to track the experiments and fine-tuning progress.","metadata":{}},{"cell_type":"code","source":"import wandb\n\nwb_token = user_secrets.get_secret(\"wnb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune-DeepSeek-R1-Distill-Llama-8B on Medical COT Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:05:41.883423Z","iopub.execute_input":"2025-03-10T22:05:41.883716Z","iopub.status.idle":"2025-03-10T22:05:55.515199Z","shell.execute_reply.started":"2025-03-10T22:05:41.883694Z","shell.execute_reply":"2025-03-10T22:05:55.514339Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mryan-wibawa\u001b[0m (\u001b[33mryan-wibawa-california-state-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250310_220549-5aprhx6b</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ryan-wibawa-california-state-university/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset/runs/5aprhx6b' target=\"_blank\">misty-breeze-6</a></strong> to <a href='https://wandb.ai/ryan-wibawa-california-state-university/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ryan-wibawa-california-state-university/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset' target=\"_blank\">https://wandb.ai/ryan-wibawa-california-state-university/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ryan-wibawa-california-state-university/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset/runs/5aprhx6b' target=\"_blank\">https://wandb.ai/ryan-wibawa-california-state-university/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset/runs/5aprhx6b</a>"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"## 2. Loading the model and tokenizer\nFor this project, we are loading the Unsloth version of [DeepSeek-R1-Distill-Llama-8B](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B).  Additionally, we will load the model in 4-bit quantization to optimize memory usage and performance.","metadata":{}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\n\nmax_seq_length = 2048 \ndtype = None \nload_in_4bit = True\n\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    token = hf_token, \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:06:29.264665Z","iopub.execute_input":"2025-03-10T22:06:29.264967Z","iopub.status.idle":"2025-03-10T22:07:07.033675Z","shell.execute_reply.started":"2025-03-10T22:06:29.264948Z","shell.execute_reply":"2025-03-10T22:07:07.032772Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.3.9: Fast Llama patching. Transformers: 4.49.0.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c15e67f7c0b44378b373cf6117ddee3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a2c5825d7314e798718ae6716ba0114"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"509b90449f7c45fdb7f85901a9c8ecf3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0da4873b8bfd43ecbef96812189aa170"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47936eab68a84b9997232196786e3188"}},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"## 3. Model inference before fine-tuning\nTo create a prompt style for the model, we will define a system prompt and include placeholders for the question and response generation. The prompt will guide the model to think step-by-step and provide a logical, accurate response.","metadata":{}},{"cell_type":"code","source":"prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:07:32.099522Z","iopub.execute_input":"2025-03-10T22:07:32.099955Z","iopub.status.idle":"2025-03-10T22:07:32.105471Z","shell.execute_reply.started":"2025-03-10T22:07:32.099923Z","shell.execute_reply":"2025-03-10T22:07:32.104169Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"In this example, we will provide a medical question to the prompt_style, convert it into tokens, and then pass the tokens to the model for response generation. ","metadata":{}},{"cell_type":"code","source":"question = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\n\n\nFastLanguageModel.for_inference(model) \ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:07:41.202455Z","iopub.execute_input":"2025-03-10T22:07:41.202750Z","iopub.status.idle":"2025-03-10T22:08:39.135642Z","shell.execute_reply.started":"2025-03-10T22:07:41.202730Z","shell.execute_reply":"2025-03-10T22:08:39.134747Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nOkay, so I'm trying to figure out what the cystometry would show for this 61-year-old woman. Let me start by breaking down the information given.\n\nFirst, the patient has a history of involuntary urine loss when she coughs or sneezes. That makes me think about stress urinary incontinence. I remember that stress incontinence is usually due to the bladder not fully emptying, especially when there's increased abdominal pressure, like from coughing. But she doesn't leak at night, which is interesting because that suggests it's not a mixed urinary incontinence case, where both stress and urge components are present.\n\nShe underwent a gynecological exam and a Q-tip test. I'm a bit rusty on the Q-tip test, but I think it's used to check for urethral obstruction. If the Q-tip (a small catheter) is difficult to insert or stays in the urethra, it might indicate that the urethral sphincter is not functioning properly, leading to obstruction. But I'm not entirely sure how that relates to the bladder's function.\n\nNow, considering the possible findings in cystometry. Cystometry is a test where they fill the bladder and measure how much it holds (residual volume) and the pressure at which it contracts (detrusor pressure). If there's stress incontinence, I expect the residual volume to be low because the bladder doesn't hold much urine. But during a cough or sneeze, the increased abdominal pressure would cause the urethral sphincter to open, leading to leakage.\n\nWait, but if the Q-tip test shows urethral obstruction, that would mean the urethral sphincter isn't opening properly. That could lead to retention of urine because the sphincter can't relax. So, in that case, the residual volume in the bladder might be high because the sphincter isn't opening, and the bladder can't empty. But the patient doesn't leak at night, so maybe she has some other issue.\n\nAlternatively, if the Q-tip test is normal, meaning the urethral sphincter opens properly, then the issue is more likely with the bladder's capacity. So, in the cystometry, the residual volume would be low because the bladder doesn't hold much, and the detrusor pressure would be normal or maybe elevated because it contracts quickly to prevent incontinence.\n\nI think I'm mixing up a few possibilities here. Let me try to organize my thoughts:\n\n1. Stress urinary incontinence: Bladder doesn't empty completely, so residual volume is low.\n2. Urethral obstruction: Sphincter doesn't open, leading to retention; residual volume would be high, but she doesn't leak, so maybe her bladder is also not emptying because of sphincter issue.\n3. Normal Q-tip test: Sphincter opens normally, so the issue is with the bladder's capacity.\n\nBut since she doesn't leak at night, it's more likely that the sphincter opens properly, so the problem is with the bladder not emptying (stress incontinence). So in cystometry, the residual volume is low, and detrusor pressure is normal because the bladder contracts well to prevent incontinence during activities.\n\nWait, but if the detrusor pressure is too high, that might cause the bladder to contract too strongly, leading to incontinence. So maybe the detrusor contractions are normal, but the residual volume is low because the bladder doesn't hold much.\n\nI think I've got it now. The cystometry would show a low residual volume because the bladder doesn't hold much, and normal detrusor contractions because the sphincter opens properly, leading to stress incontinence during activities like coughing.\n</think>\n\nThe cystometry findings for the 61-year-old woman would most likely reveal a **low residual volume** in the bladder and **normal detrusor contractions**. This is consistent with a diagnosis of stress urinary incontinence, where the bladder does not fully empty, leading to involuntary leakage during activities that increase abdominal pressure, such as coughing or sneezing. The normal detrusor contractions indicate that the sphincter opens properly, contributing to the stress component of incontinence. The absence of leakage at night suggests that the primary issue is not with nighttime bladder capacity but rather with the sphincter's ability to prevent leakage during the day.<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"Even without fine-tuning, our model successfully generated a chain of thought and provided reasoning before delivering the final answer. The reasoning process is encapsulated within the <think></think> tags.\n\nSo, why do we still need fine-tuning? The reasoning process, while detailed, was long-winded and not concise. Additionally, the final answer was presented in a bullet-point format, which deviates from the structure and style of the dataset that we want to fine-tune on. \n\n## 4. Loading and processing the dataset\nWe will slightly change the prompt style for processing the dataset by adding the third placeholder for the complex chain of thought column. ","metadata":{}},{"cell_type":"code","source":"train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n</think>\n{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:08:53.636644Z","iopub.execute_input":"2025-03-10T22:08:53.636934Z","iopub.status.idle":"2025-03-10T22:08:53.641604Z","shell.execute_reply.started":"2025-03-10T22:08:53.636913Z","shell.execute_reply":"2025-03-10T22:08:53.640795Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"Write the Python function that will create a \"text\" column in the dataset, which consists of the train prompt style. Fill the placeholders with questions, chains of text, and answers. ","metadata":{}},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n\n\ndef formatting_prompts_func(examples):\n    inputs = examples[\"Question\"]\n    cots = examples[\"Complex_CoT\"]\n    outputs = examples[\"Response\"]\n    texts = []\n    for input, cot, output in zip(inputs, cots, outputs):\n        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n        texts.append(text)\n    return {\n        \"text\": texts,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:09:09.332754Z","iopub.execute_input":"2025-03-10T22:09:09.333067Z","iopub.status.idle":"2025-03-10T22:09:09.339391Z","shell.execute_reply.started":"2025-03-10T22:09:09.333045Z","shell.execute_reply":"2025-03-10T22:09:09.338296Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"We will load the first 500 samples from the [FreedomIntelligence/medical-o1-reasoning-SFT](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT?row=46) dataset, which is available on the Hugging Face hub. After that, we will map the text column using the formatting_prompts_func function.\n\nAs we can see, the text column has a system prompt, instructions, chain of thought, and the answer. ","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split = \"train[0:500]\",trust_remote_code=True)\ndataset = dataset.map(formatting_prompts_func, batched = True,)\ndataset[\"text\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:09:23.173533Z","iopub.execute_input":"2025-03-10T22:09:23.173845Z","iopub.status.idle":"2025-03-10T22:09:28.403435Z","shell.execute_reply.started":"2025-03-10T22:09:23.173823Z","shell.execute_reply":"2025-03-10T22:09:28.402731Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33b4b779135742618e0e75187e65ddc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"medical_o1_sft.json:   0%|          | 0.00/74.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43ab16c999f54c29bba0aeb33bf213e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25371 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c74709b3a454cca861bd93609f4cd94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45a85f7087a04466896defa8107d9fcc"}},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"\"Below is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \\nPlease answer the following medical question. \\n\\n### Question:\\nA 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\\n\\n### Response:\\n<think>\\nOkay, let's think about this step by step. There's a 61-year-old woman here who's been dealing with involuntary urine leakages whenever she's doing something that ups her abdominal pressure like coughing or sneezing. This sounds a lot like stress urinary incontinence to me. Now, it's interesting that she doesn't have any issues at night; she isn't experiencing leakage while sleeping. This likely means her bladder's ability to hold urine is fine when she isn't under physical stress. Hmm, that's a clue that we're dealing with something related to pressure rather than a bladder muscle problem. \\n\\nThe fact that she underwent a Q-tip test is intriguing too. This test is usually done to assess urethral mobility. In stress incontinence, a Q-tip might move significantly, showing urethral hypermobility. This kind of movement often means there's a weakness in the support structures that should help keep the urethra closed during increases in abdominal pressure. So, that's aligning well with stress incontinence.\\n\\nNow, let's think about what would happen during cystometry. Since stress incontinence isn't usually about sudden bladder contractions, I wouldn't expect to see involuntary detrusor contractions during this test. Her bladder isn't spasming or anything; it's more about the support structure failing under stress. Plus, she likely empties her bladder completely because stress incontinence doesn't typically involve incomplete emptying. So, her residual volume should be pretty normal. \\n\\nAll in all, it seems like if they do a cystometry on her, it will likely show a normal residual volume and no involuntary contractions. Yup, I think that makes sense given her symptoms and the typical presentations of stress urinary incontinence.\\n</think>\\nCystometry in this case of stress urinary incontinence would most likely reveal a normal post-void residual volume, as stress incontinence typically does not involve issues with bladder emptying. Additionally, since stress urinary incontinence is primarily related to physical exertion and not an overactive bladder, you would not expect to see any involuntary detrusor contractions during the test.<｜end▁of▁sentence｜>\""},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"## 5. Setting up the model\nUsing the target modules, we will set up the model by adding the low-rank adopter to the model. ","metadata":{}},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r=16,  \n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n    ],\n    lora_alpha=16,\n    lora_dropout=0,  \n    bias=\"none\",  \n    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n    random_state=3407,\n    use_rslora=False,  \n    loftq_config=None,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:09:46.669888Z","iopub.execute_input":"2025-03-10T22:09:46.670352Z","iopub.status.idle":"2025-03-10T22:09:53.082217Z","shell.execute_reply.started":"2025-03-10T22:09:46.670315Z","shell.execute_reply":"2025-03-10T22:09:53.081533Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.3.9 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"Next, we will set up the training arguments and the trainer by providing the model, tokenizers, dataset, and other important training parameters that will optimize our fine-tuning process.","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n        warmup_steps=5,\n        max_steps=60,\n        learning_rate=2e-4,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=10,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\",\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:10:15.467208Z","iopub.execute_input":"2025-03-10T22:10:15.467536Z","iopub.status.idle":"2025-03-10T22:10:18.214936Z","shell.execute_reply.started":"2025-03-10T22:10:15.467512Z","shell.execute_reply":"2025-03-10T22:10:18.214221Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Tokenizing to [\"text\"] (num_proc=2):   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9536aedce6ff41e9a4d770dca6c38422"}},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"## 6. Model training\nRun the following command to start training.  ","metadata":{}},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T22:10:27.316996Z","iopub.execute_input":"2025-03-10T22:10:27.317335Z","iopub.status.idle":"2025-03-10T22:49:32.073980Z","shell.execute_reply.started":"2025-03-10T22:10:27.317306Z","shell.execute_reply":"2025-03-10T22:49:32.073375Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 500 | Num Epochs = 2 | Total steps = 60\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n \"-____-\"     Trainable parameters = 41,943,040/4,670,623,744 (0.90% trained)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 38:06, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.914200</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.421000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.366000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.328500</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.295100</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.326900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"You can view the fill model evaluation report on the Weights and biases dash board by logging into the website and viewing the [project](https://wandb.ai/ryan-wibawa-california-state-university/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset?nw=nwuserryanwibawa).\n\n## 7. Model inference after fine-tuning\nTo compare the results, we will ask the fine-tuned model the same question as before to see what has changed.","metadata":{}},{"cell_type":"code","source":"question = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\n\n\nFastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)\nprint(response[0].split(\"### Response:\")[1])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T23:08:32.732699Z","iopub.execute_input":"2025-03-10T23:08:32.733029Z","iopub.status.idle":"2025-03-10T23:09:08.846702Z","shell.execute_reply.started":"2025-03-10T23:08:32.733006Z","shell.execute_reply":"2025-03-10T23:09:08.845694Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nOkay, let's see. We have a 61-year-old woman with a history of involuntary urine loss, especially during things like coughing or sneezing. That sounds like a classic presentation of urinary incontinence, probably stress incontinence. Now, stress incontinence usually means the bladder neck isn't closing properly, and when the abdominal muscles contract, the bladder doesn't hold up.\n\nNow, during a gynecological exam, they would check the cervix and bladder neck. The Q-tip test is commonly used in stress incontinence. If the Q-tip is positive, that means the bladder neck is not closing properly when the abdominal muscles contract. This makes sense because stress incontinence is all about the bladder neck not sealing.\n\nWith stress incontinence, I'm expecting that when we do cystometry, the residual volume might not be too bad. But we need to check what happens during a cough or sneeze. The detrusor contractions, which are like the muscle contractions in the bladder, are usually normal or even decreased in stress incontinence. It's not like they're overactive or something. So, when we look at the bladder's contractions during a cough, they might not be very strong, which means the bladder doesn't contract fully and the sphincter doesn't hold.\n\nSo, putting it all together, with stress incontinence and a positive Q-tip test, the cystometry results are likely to show that the bladder doesn't contract much during a cough. The residual volume is probably not too high, and the detrusor contractions are not strong enough to prevent the urine from leaking out.\n\nYeah, that all makes sense. Stress incontinence is all about the bladder neck not holding up, and the cystometry would show that the bladder isn't contracting well during a cough or sneeze. So, the findings are pointing towards normal residual volume and decreased detrusor contractions in this case.\n</think>\nBased on the presentation of urinary incontinence, particularly during activities like coughing or sneezing, and the positive Q-tip test indicating a failure of the bladder neck to close properly, the findings of the cystometry would most likely reveal a normal residual volume and decreased detrusor contractions. In stress incontinence, the bladder is unable to contract sufficiently during a cough, leading to leakage. The detrusor contractions, which are usually normal or decreased in stress incontinence, are not strong enough to prevent the leakage of urine. Therefore, the cystometry results would indicate that the bladder does not contract strongly during a cough or sneeze, with normal residual volume.<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"This is much better and more accurate. The chain of thought was direct, and the answer was straightforward and in one paragraph. The fine-tuning was successful.\n\n## 8. Saving the model locally\nNow, let's save the adopter, full model, and tokenizer locally so that we can use them in other projects.","metadata":{}},{"cell_type":"code","source":"new_model_local = \"DeepSeek-R1-Medical-COT\"\nmodel.save_pretrained(new_model_local) \ntokenizer.save_pretrained(new_model_local)\n\nmodel.save_pretrained_merged(new_model_local, tokenizer, save_method = \"merged_16bit\",)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T23:11:04.525588Z","iopub.execute_input":"2025-03-10T23:11:04.525913Z","iopub.status.idle":"2025-03-10T23:13:07.275270Z","shell.execute_reply.started":"2025-03-10T23:11:04.525890Z","shell.execute_reply":"2025-03-10T23:13:07.274290Z"}},"outputs":[{"name":"stderr","text":"Unsloth: You have 2 CPUs. Using `safe_serialization` is 10x slower.\nWe shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\nTo force `safe_serialization`, set it to `None` instead.\nUnsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\nmodel which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\nUnsloth: Will remove a cached repo with size 6.0G\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 16.08 out of 31.35 RAM for saving.\nUnsloth: Saving model... This might take 5 minutes ...\n","output_type":"stream"},{"name":"stderr","text":" 34%|███▍      | 11/32 [00:00<00:01, 15.12it/s]\nWe will save to Disk and not RAM now.\n100%|██████████| 32/32 [00:26<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Saving tokenizer... Done.\nUnsloth: Saving DeepSeek-R1-Medical-COT/pytorch_model-00001-of-00004.bin...\nUnsloth: Saving DeepSeek-R1-Medical-COT/pytorch_model-00002-of-00004.bin...\nUnsloth: Saving DeepSeek-R1-Medical-COT/pytorch_model-00003-of-00004.bin...\nUnsloth: Saving DeepSeek-R1-Medical-COT/pytorch_model-00004-of-00004.bin...\nDone.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"## 9. Pushing the model to Hugging Face Hub\nWe will also push the adopter, tokenizer, and model to Hugging Face Hub so that the AI community can take advantage of this model by integrating it into their systems.","metadata":{}},{"cell_type":"code","source":"new_model_online = \"rwibawa/DeepSeek-R1-Medical-COT\"\nmodel.push_to_hub(new_model_online)\ntokenizer.push_to_hub(new_model_online)\n\nmodel.push_to_hub_merged(new_model_online, tokenizer, save_method = \"merged_16bit\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T23:38:20.681553Z","iopub.execute_input":"2025-03-10T23:38:20.681900Z","iopub.status.idle":"2025-03-10T23:43:33.256396Z","shell.execute_reply.started":"2025-03-10T23:38:20.681871Z","shell.execute_reply":"2025-03-10T23:43:33.255449Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/626 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c536c6ffddc94f7c8bedd396f14b1bc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c7223b3d0dc44c3aa2f522e473cafc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bdf981906e4443ab6695e80cb206b33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62fb9b05c5504f48864cae105577e73e"}},"metadata":{}},{"name":"stdout","text":"Saved model to https://huggingface.co/rwibawa/DeepSeek-R1-Medical-COT\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41a0bbdee2e44eb8af08f477781dd227"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43a994c1f8be42d48cd88a9dd29a2bf4"}},"metadata":{}},{"name":"stderr","text":"Unsloth: You are pushing to hub in Kaggle environment.\nTo save memory, we shall move rwibawa/DeepSeek-R1-Medical-COT to /tmp/DeepSeek-R1-Medical-COT\nUnsloth: Will remove a cached repo with size 1.6K\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 15.26 out of 31.35 RAM for saving.\nUnsloth: Saving model... This might take 5 minutes ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 32/32 [00:26<00:00,  1.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Saving tokenizer... Done.\nUnsloth: Saving /tmp/DeepSeek-R1-Medical-COT/pytorch_model-00001-of-00004.bin...\nUnsloth: Saving /tmp/DeepSeek-R1-Medical-COT/pytorch_model-00002-of-00004.bin...\nUnsloth: Saving /tmp/DeepSeek-R1-Medical-COT/pytorch_model-00003-of-00004.bin...\nUnsloth: Saving /tmp/DeepSeek-R1-Medical-COT/pytorch_model-00004-of-00004.bin...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2634c8bba7054482a647e2654e849140"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00004.bin:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b17264629e144226b9387deb3b8b7fd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00004.bin:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c5de70a238544d882721d5fec782c75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00003-of-00004.bin:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"455160353b3f4a1dbf6db9f2bc05e236"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00004-of-00004.bin:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"952c31b446e5446e88e535e44fba736e"}},"metadata":{}},{"name":"stdout","text":"Done.\nSaved merged model to https://huggingface.co/rwibawa/DeepSeek-R1-Medical-COT\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"The next step in your learning journey is to serve and deploy your model to the cloud. You can follow the [How to Deploy LLMs with BentoML](https://www.datacamp.com/tutorial/deploy-llms-with-bentoml) guide, which provides a step-by-step process for deploying large language models efficiently and cost-effectively using BentoML and tools like vLLM.\n\nAlternatively, if you prefer to use the model locally, you can convert it into GGUF format and run it on your machine. For this, check out the [Fine-tuning Llama 3.2 and Using It Locally](https://www.datacamp.com/tutorial/fine-tuning-llama-3-2) guide, which provides detailed instructions for local usage.\n\n\n## Conclusion\nOpen-source large language models (LLMs) are becoming better, faster, and more efficient, making it easier than ever to fine-tune them on lower compute and memory resources.\n\nIn this tutorial, we explored the DeepSeek R1 reasoning model and learned how to fine-tune its distilled version for medical Q&A tasks. A fine-tuned reasoning model not only enhances performance but also enables its application in critical fields such as medicine, emergency services, and healthcare.","metadata":{}}]}